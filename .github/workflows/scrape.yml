name: Scheduled Scraper

on:
  schedule:
    - cron: '0 */8 * * *'  # Every 8 hours (0:00, 8:00, 16:00 UTC)
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 90  # Increased for cloud scraping
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Install Playwright browsers
        run: playwright install chromium --with-deps
          
      - name: Run scraper
        env:
          USE_TIDB: 'true'
          TIDB_HOST: ${{ secrets.TIDB_HOST }}
          TIDB_PORT: ${{ secrets.TIDB_PORT }}
          TIDB_USER: ${{ secrets.TIDB_USER }}
          TIDB_PASSWORD: ${{ secrets.TIDB_PASSWORD }}
          TIDB_DATABASE: ${{ secrets.TIDB_DATABASE }}
        run: |
          echo "Starting scraper at $(date)"
          python scrape_all.py
          echo "Scraper completed at $(date)"
          
      - name: Verify database
        env:
          USE_TIDB: 'true'
          TIDB_HOST: ${{ secrets.TIDB_HOST }}
          TIDB_PORT: ${{ secrets.TIDB_PORT }}
          TIDB_USER: ${{ secrets.TIDB_USER }}
          TIDB_PASSWORD: ${{ secrets.TIDB_PASSWORD }}
          TIDB_DATABASE: ${{ secrets.TIDB_DATABASE }}
        run: |
          python -c "
          from database.tidb_manager import TiDBManager
          db = TiDBManager()
          stats = db.get_statistics()
          print(f'Database Stats: {stats[\"total_events\"]} total events')
          print(f'By Source: {stats[\"by_source\"]}')
          "
